import streamlit as st
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama

from utils import print_message

st.set_page_config(page_title="ë‚˜ë§Œì˜ Translator ğŸ’¬", page_icon="ğŸ’¬")

# ë©”ì‹œì§€ ì €ì¥ìš© ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# í™”ë©´ ê°±ì‹  ìœ ë„ìš© í”Œë˜ê·¸
if "reset_flag" not in st.session_state:
    st.session_state["reset_flag"] = False

# UI ìƒë‹¨: íƒ€ì´í‹€ ë° ë¦¬ì…‹ ë²„íŠ¼
with st.container():
    st.title("ë‚˜ë§Œì˜ Translator ğŸ’¬")
    if st.button("ğŸ”„ ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.session_state["reset_flag"] = True

# ë¦¬ì…‹ í”Œë˜ê·¸ê°€ ì¼œì ¸ ìˆìœ¼ë©´ ì²˜ë¦¬ í›„ ë”
if st.session_state["reset_flag"]:
    st.session_state["reset_flag"] = False
    # ì¶”ê°€ì ì¸ ë¦¬ì…‹ ì‹œ ì²˜ë¦¬ ë¡œì§ ê°€ëŠ¥ (í˜„ì¬ëŠ” ë©”ì‹œì§€ ë¹„ì›€ìœ¼ë¡œ ì¶©ë¶„)

# LLM ê°ì²´ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)
llm = ChatOllama(model="llama3.1:8b", temperature=0)

# Prompt í…œí”Œë¦¿ ì •ì˜
prompt_template_str = """
ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
{question}
"""
prompt_template = ChatPromptTemplate.from_template(prompt_template_str)

# ê¸°ì¡´ ëŒ€í™” ë©”ì‹œì§€ ì¶œë ¥
print_message()

# ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸° ë° ì²˜ë¦¬
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í™”ë©´ì— ì¶œë ¥
    st.chat_message("user").write(user_input)
    user_msg = ChatMessage(role="user", content=user_input)
    st.session_state["messages"].append(user_msg)

    # LLM ì‹¤í–‰
    output = llm.invoke(prompt_template.format_prompt(question=user_input).to_string())

    # AI ì‘ë‹µ ì €ì¥ ë° í™”ë©´ ì¶œë ¥
    with st.chat_message("assistant"):
        st.write(output.content)
        assistant_msg = ChatMessage(role="assistant", content=output.content)
        st.session_state["messages"].append(assistant_msg)
