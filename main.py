import streamlit as st
from langchain_core.messages import ChatMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama

from utils import print_message

st.set_page_config(page_title="ë‚˜ë§Œì˜ Translator ğŸ’¬", page_icon="ğŸ’¬")
st.title("ë‚˜ë§Œì˜ Translator ğŸ’¬")

# ì„¸ì…˜ ìƒíƒœ ë©”ì‹œì§€ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# Ollama LLM ê°ì²´ í•œ ë²ˆë§Œ ì´ˆê¸°í™”
llm = ChatOllama(model="llama3.2:3b", temperature=0)

# Prompt í…œí”Œë¦¿ ì •ì˜
prompt_template_str = """
ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
{question}
"""
prompt_template = ChatPromptTemplate.from_template(prompt_template_str)


user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

print_message()  # ê¸°ì¡´ ë©”ì‹œì§€ í™”ë©´ì— ì¶œë ¥

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í™”ë©´ ì¶œë ¥
    st.chat_message("user").write(user_input)
    user_msg = ChatMessage(role="user", content=user_input)
    st.session_state["messages"].append(user_msg)

    # prompt í…œí”Œë¦¿ì— ì§ˆë¬¸ ë³€ìˆ˜ í• ë‹¹í•˜ì—¬ ì²´ì¸ í˜¸ì¶œ
    output = llm.invoke(prompt_template.format_prompt(question=user_input).to_string())

    # AI ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥ ë° í™”ë©´ ì¶œë ¥
    with st.chat_message("assistant"):
        st.write(output.content)
        assistant_msg = ChatMessage(role="assistant", content=output.content)
        st.session_state["messages"].append(assistant_msg)
